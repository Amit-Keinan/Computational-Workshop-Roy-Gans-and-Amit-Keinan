{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2447999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import  GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score ,make_scorer, mean_squared_error\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f40fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multi_index_data(path):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame with paired identifiers into a MultiIndex DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Data matrix with first 2 columns representing index identifiers, and the remaining clumns contain data values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : pd.DataFrame\n",
    "        A DataFrame with a MultiIndex (subj_i, subj_j) as the index and the remaining columns as data.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path)\n",
    "    multi_index = pd.MultiIndex.from_arrays(data.iloc[:,:2].values.T ,names=['subj_i', 'subj_j'])\n",
    "    data = data.iloc[:,2:]\n",
    "    data.index = multi_index\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9650c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(feat, label, sampling_method='over', random_state=0, sampling_strategy='auto', **kargs):\n",
    "    \"\"\"\n",
    "    Resample a dataset to address class imbalance using various sampling techniques.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feat : np.array\n",
    "        Features of the dataset to resample.\n",
    "    label : np.array\n",
    "        Target class labels.\n",
    "    sampling_method : str, default='over'\n",
    "        Resampling technique to use:\n",
    "        - 'under': Random undersampling\n",
    "        - 'over': Random oversampling\n",
    "    random_state : int, default=0\n",
    "        Random seed for reproducibility.\n",
    "    sampling_strategy : str or dict, default='auto'\n",
    "        Sampling strategy to use.\n",
    "    **kargs : dict\n",
    "        Additional parameters to pass to the specific resampling algorithm.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    feat_resampled : np.array\n",
    "        Resampled feature data.\n",
    "    label_resampled : np.array\n",
    "        Resampled target labels.\n",
    "    \"\"\"\n",
    "\n",
    "    sampler_dict = {\n",
    "        'under': RandomUnderSampler,\n",
    "        'over': RandomOverSampler,\n",
    "        }\n",
    "\n",
    "    # Validate input parameters\n",
    "    valid_methods = ['under', 'over']\n",
    "    if sampling_method not in valid_methods:\n",
    "        raise ValueError(f\"Invalid value for 'sampling_method'. Use one of {valid_methods}\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    sampler = sampler_dict[sampling_method](sampling_strategy=sampling_strategy, random_state=random_state, **kargs)\n",
    "    feat_resampled, label_resampled = sampler.fit_resample(feat, label)\n",
    "    \n",
    "    return feat_resampled, label_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01f35658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_se(metric_list):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard error of a list of metric values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    metric_list : list of float\n",
    "        List of metric values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean : float\n",
    "        The mean of the metric values.\n",
    "    se : float\n",
    "        The standard error of the values.\n",
    "\"\"\"\n",
    "    mean = np.mean(metric_list)\n",
    "    se = np.std(metric_list) / np.sqrt(len(metric_list))\n",
    "\n",
    "    return mean, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c54b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_classifier(feat, label, model, grid, scoring, to_resample=None, cv=10):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a classifier using k-fold cross-validation with optional resampling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feat : DataFrame\n",
    "        Feature matrix.\n",
    "    label : array-like\n",
    "        Target vector with class labels.\n",
    "    model : estimator object\n",
    "        Classifier model (default: LinearSVC).\n",
    "    grid : dict\n",
    "        Parameter grid for GridSearchCV.\n",
    "    scoring : str\n",
    "        Scoring metric for GridSearchCV.\n",
    "    to_resample : str or None\n",
    "        Resampling method ('under', 'over', 'smote', or None).\n",
    "    cv : int\n",
    "        Number of cross-validation folds.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing various metrics and predictions.\n",
    "    \"\"\"\n",
    "    splitter = StratifiedKFold(n_splits=cv, shuffle=True, random_state=0)\n",
    "    prediction = np.zeros((feat.shape[0]))\n",
    "    acc_list, balanced_acc_list, f1_weighted_list, f1_macro_list = [], [], [], []\n",
    "    n_classes = len(np.unique(label))\n",
    "    mean_confusion_matrix = np.zeros((n_classes, n_classes))\n",
    "    best_params_list = []\n",
    "\n",
    "    # Encode class labels into numeric values\n",
    "    le = LabelEncoder()\n",
    "    label = le.fit_transform(label)\n",
    "\n",
    "    # Get relevant data slices for train-test split\n",
    "    for fold, (train_idx, test_idx) in enumerate(splitter.split(feat, label)):\n",
    "        feat_train = feat.iloc[train_idx, :].to_numpy()\n",
    "        feat_test = feat.iloc[test_idx, :].to_numpy()\n",
    "        label_train = label[train_idx]\n",
    "        label_test = label[test_idx]\n",
    "\n",
    "        # Resample the training set to address class imbalance if requested \n",
    "        if to_resample:\n",
    "            feat_train, label_train = resampling(feat_train, label_train, to_resample)\n",
    "\n",
    "        # Standardize the training and test feature sets\n",
    "        scaler = StandardScaler()\n",
    "        feat_train = scaler.fit_transform(feat_train)\n",
    "        feat_test = scaler.transform(feat_test) # Apply scaling params learned from train\n",
    "        \n",
    "        # Performs an inner grid search with cross validation to find and train the best model parameters for this fold\n",
    "        clf = GridSearchCV(model, grid, cv=10, scoring=scoring, n_jobs=-1)\n",
    "        clf.fit(feat_train, label_train)\n",
    "        best_estimator = clf.best_estimator_\n",
    "        best_params_list.append(clf.best_params_)\n",
    "\n",
    "        print(f'Best parameters: {clf.best_params_}')\n",
    "\n",
    "        # Make prediction on test set\n",
    "        label_predicted_test = best_estimator.predict(feat_test)\n",
    "        prediction[test_idx] = label_predicted_test\n",
    "\n",
    "        # Compute test set performance metrics: accuracy, balanced accuracy, weighted F1, and macro F1.\n",
    "        acc_test = accuracy_score(label_test, label_predicted_test)\n",
    "        balanced_acc_test = balanced_accuracy_score(label_test, label_predicted_test)\n",
    "        f1_weighted_test = f1_score(label_test, label_predicted_test, average='weighted')\n",
    "        f1_macro_test = f1_score(label_test, label_predicted_test, average='macro')\n",
    "\n",
    "        # Append the test set metrics of this fold to their respective lists\n",
    "        acc_list.append(acc_test)\n",
    "        balanced_acc_list.append(balanced_acc_test)\n",
    "        f1_weighted_list.append(f1_weighted_test)\n",
    "        f1_macro_list.append(f1_macro_test)\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        fold_confusion_mat = confusion_matrix(label_test, label_predicted_test, normalize='true')\n",
    "        mean_confusion_matrix += fold_confusion_mat\n",
    "\n",
    "    # Average the confusion matrix\n",
    "    mean_confusion_matrix /= cv\n",
    "    \n",
    "    # Calculate mean and standard error for metrics\n",
    "    mean_acc, se_acc = get_mean_and_se(acc_list)\n",
    "    mean_balanced_acc, se_balanced_acc = get_mean_and_se(balanced_acc_list)\n",
    "    mean_f1_weighted, se_f1_weighted = get_mean_and_se(f1_weighted_list)\n",
    "    mean_f1_macro, se_f1_macro = get_mean_and_se(f1_macro_list)\n",
    "\n",
    "    # Print summary\n",
    "    print('\\n=== SUMMARY ===')\n",
    "    print(f'Mean Accuracy: {mean_acc:.2f} ± {se_acc:.2f}')\n",
    "    print(f'Mean Balanced Accuracy: {mean_balanced_acc:.2f} ± {se_balanced_acc:.2f}')\n",
    "    print(f'Mean F1 Weighted: {mean_f1_weighted:.2f} ± {se_f1_weighted:.2f}')\n",
    "    print(f'Mean F1 Macro: {mean_f1_macro:.2f} ± {se_f1_macro:.2f}')\n",
    "\n",
    "    # Return results and additional metrics\n",
    "    results_summary = {\n",
    "        'predictions': prediction,\n",
    "        'accuracy': {'mean': mean_acc, 'se': se_acc},\n",
    "        'balanced_accuracy': {'mean': mean_balanced_acc, 'se': se_balanced_acc},\n",
    "        'f1_weighted': {'mean': mean_f1_weighted, 'se': se_f1_weighted},\n",
    "        'f1_macro': {'mean': mean_f1_macro, 'se': se_f1_macro},\n",
    "        'confusion_matrix': mean_confusion_matrix,\n",
    "        'best_params': best_params_list,\n",
    "    }\n",
    "\n",
    "    folds_results = {\n",
    "        'accuracy': acc_list,\n",
    "        'balanced_accuracy': balanced_acc_list,\n",
    "        'f1_weighted': f1_weighted_list,\n",
    "        'f1_macro': f1_macro_list,\n",
    "    }\n",
    "    \n",
    "    return results_summary, folds_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b277e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(confusion_mat, categories, vmin=0, vmax=1, ax=None):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the confusion matrix from the given results.\n",
    "\n",
    "    Parameters \n",
    "    ----------\n",
    "    confusion_mat: \n",
    "        matrix to plot.\n",
    "    categories : list of str\n",
    "        list of category labels for x and y axes.\n",
    "    vmin : float\n",
    "        Minimum value for colormap normalization.\n",
    "    vmax : float\n",
    "        Maximum value for colormap normalization.\n",
    "    ax : matplotlib.axes.Axes\n",
    "        matplotlib axis object. If not provided, a new figure and axis will be created.\n",
    "\n",
    "    Returns:\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axis with the heatmap plotted.\n",
    "    \"\"\"\n",
    "    # Set font to Calibri\n",
    "    calibri_font_path = '/Volumes/homes/Maya/Code/python/Calibri/Calibri.ttf'\n",
    "    fm.fontManager.addfont(calibri_font_path)\n",
    "    plt.rcParams['font.family'] = 'Calibri'\n",
    "\n",
    "    # Create figure and axes if not provided\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))  # Adjust size as needed\n",
    "\n",
    "    # Define and create plot heatmap\n",
    "    sns.heatmap(confusion_mat, \n",
    "                ax=ax, \n",
    "                xticklabels=categories, \n",
    "                yticklabels=categories, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='coolwarm', \n",
    "                cbar=True, \n",
    "                square=True, \n",
    "                vmin=vmin, vmax=vmax,\n",
    "                annot_kws={\"size\": 16})\n",
    "    \n",
    "    ax.set_xlabel('Predicted', fontsize=16)  \n",
    "    ax.set_ylabel('Actual', fontsize=16)\n",
    "    ax.set_title('Confusion Matrix', fontsize=18) \n",
    "    ax.tick_params(axis='both', labelsize=14) \n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa97562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_SVM(grid):\n",
    "    # Load data\n",
    "    behav_path = '/Volumes/homes/Maya/students_study/friendships/shortest_path_length/my_sample/second_time_point_social_distance_baseline_subj.csv'\n",
    "    behav_data = read_multi_index_data(behav_path)\n",
    "    label = behav_data.astype(int).values.ravel()\n",
    "    categories = list(range(1, int(np.max(label)) + 1))\n",
    "    # Define model\n",
    "    to_resample = 'over' \n",
    "    model = SVC()\n",
    "    scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "    # Define matrix to store results \n",
    "    predictions_per_time_point = np.zeros((98,2))\n",
    "\n",
    "    for movie_type in ['social']: #'average_across_movies', 'social', 'academic', 'neutral'\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        for i, time_point in enumerate(['first_time_point','second_time_point']): \n",
    "            print(f'\\n{movie_type}')\n",
    "            print(time_point)\n",
    "\n",
    "            # Load features\n",
    "            feat_path = f'/Volumes/homes/Maya/students_study/friendships/ISC/Schaefer100/{time_point}/{movie_type}_preproc_isc_data_baseline_subj.csv'\n",
    "            feat = read_multi_index_data(feat_path)\n",
    "\n",
    "            # Run model\n",
    "            results_summary, folds_results = my_classifier(feat, label, model, grid, scorer, to_resample, 10)\n",
    "            predictions_per_time_point[:,i] = results_summary['predictions'] # fill result matrix\n",
    "\n",
    "            plot_confusion_matrix(results_summary['confusion_matrix'], categories, 0.2, 0.5, axes[i])\n",
    "\n",
    "        plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5ae4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM linear')\n",
    "grid_linear = {'C': np.logspace(-3, 3, 20),\n",
    "               'kernel' : ['linear']}\n",
    "run_SVM(grid_linear)\n",
    "\n",
    "print('SVM rbf')\n",
    "grid_rbf = {'C': np.logspace(-3, 3, 20),\n",
    "            'kernel' : ['rbf']}\n",
    "run_SVM(grid_rbf)\n",
    "\n",
    "print('SVM poly')\n",
    "grid_poly = {'C': np.logspace(-3, 3, 20),\n",
    "             'kernel' : ['poly']}\n",
    "run_SVM(grid_poly)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
